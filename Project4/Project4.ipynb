{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 题目\n",
    "Write a script to compute the decision tree for an input dataset.\n",
    "\n",
    "You can assume that all attributes are numeric, except for the last attribute which is the class.\n",
    "\n",
    "Use the Information Gain based on Entropy for computing the best split value for eachattribute. For the stopping criteria at a node, stop if the purity is at least 95% or stop if thenode size is five or lower.\n",
    "\n",
    "Output your decision tree. Note that each internal node, print the decision followed by thenformation Gain, and for each leaf, print the majority label, purity of the leaf, and the size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树\n",
    "决策树模型是运用于分类以及回归的一种树结构。决策树由节点和有向边组成，一般一棵决策树包含一个根节点、若干内部节点和若干叶节点。决策树的决策过程需要从决策树的根节点开始，待测数据与决策树中的特征节点进行比较，并按照比较结果选择选择下一比较分支，直到叶子节点作为最终的决策结果。\n",
    "\n",
    "目标变量可以采用一组离散值的树模型称为分类树(常用的分类树算法有ID3、C4.5、CART)，而目标变量可以采用连续值（通常是实数）的决策树被称为回归树(如CART算法)。\n",
    "\n",
    "决策树算法本质上就是要找出每一列的最佳划分以及不同列划分的先后顺序及排布。\n",
    "\n",
    "信息熵表示的是不确定度。均匀分布时，不确定度最大，此时熵就最大。当选择某个特征对数据集进行分类时，分类后的数据集信息熵会比分类前的小，其差值表示为信息增益。信息增益可以衡量某个特征对分类结果的影响大小。\n",
    "\n",
    "假设在样本数据集 D 中，混有 c 种类别的数据。构建决策树时，根据给定的样本数据集选择某个特征值作为树的节点。在数据集中，可以计算出该数据中的信息熵：![Image of Yaktocat](https://www.ibm.com/developerworks/cn/analytics/library/ba-1507-decisiontree-algorithm/img02.png)\n",
    "对应数据集 D，选择特征 A 作为决策树判断节点时，在特征 A 作用后的信息熵的为 Info(D)，计算如下：![Image of Yaktocat](https://www.ibm.com/developerworks/cn/analytics/library/ba-1507-decisiontree-algorithm/img03.png)\n",
    "信息增益表示数据集 D 在特征 A 的作用后，其信息熵减少的值。公式如下：![Image of Yaktocat](https://www.ibm.com/developerworks/cn/analytics/library/ba-1507-decisiontree-algorithm/img04.png)\n",
    "对于决策树节点最合适的特征选择，就是 Gain(A) 值最大的特征。\n",
    "\n",
    "在分类模型建立的过程中，很容易出现过拟合的现象。过拟合是指在模型学习训练中，训练样本达到非常高的逼近精度，但对检验样本的逼近误差随着训练次数而呈现出先下降后上升的现象。过拟合时训练误差很小，但是检验误差很大，不利于实际应用。\n",
    "\n",
    "决策树的过拟合现象可以通过剪枝进行一定的修复。剪枝分为预先剪枝和后剪枝两种。\n",
    "\n",
    "预先剪枝指在决策树生长过程中，使用一定条件加以限制，使得产生完全拟合的决策树之前就停止生长。预先剪枝的判断方法也有很多，比如信息增益小于一定阀值的时候通过剪枝使决策树停止生长。但如何确定一个合适的阀值也需要一定的依据，阀值太高导致模型拟合不足，阀值太低又导致模型过拟合。\n",
    "\n",
    "后剪枝是在决策树生长完成之后，按照自底向上的方式修剪决策树。后剪枝有两种方式，一种用新的叶子节点替换子树，该节点的预测类由子树数据集中的多数类决定。另一种用子树中最常使用的分支代替子树。\n",
    "\n",
    "预先剪枝可能过早的终止决策树的生长，后剪枝一般能够产生更好的效果。但后剪枝在子树被剪掉后，决策树生长的一部分计算就被浪费了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attribute': 0, 'split_point': 5.45, 'Gain': 0.5309046836085699, 'left': {'attribute': 1, 'split_point': 2.8, 'Gain': 0.35726699979283877, 'left': {'attribute': 0, 'split_point': 4.95, 'Gain': 0.19811742113040343, 'left': {'label': 2, 'purity': 0.6666666666666666, 'number': 3}, 'right': {'label': 2, 'purity': 1.0, 'number': 4}}, 'right': {'label': 1, 'purity': 0.9777777777777777, 'number': 45}}, 'right': {'attribute': 1, 'split_point': 3.45, 'Gain': 0.2142137447120585, 'left': {'label': 2, 'purity': 1.0, 'number': 89}, 'right': {'attribute': 0, 'split_point': 6.5, 'Gain': 0.9544340029249649, 'left': {'label': 1, 'purity': 1.0, 'number': 5}, 'right': {'label': 2, 'purity': 1.0, 'number': 3}}}}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "filepath = \"C:\\\\Users\\\\Shinelon\\\\Desktop\\\\data-mining  project\\\\Project4\\\\iris.txt\"\n",
    "df = pd.read_csv(filepath).values\n",
    "df=df[:,[0,1,4]]\n",
    "\n",
    "df[df[:,2]=='Iris-setosa',2]=1\n",
    "df[df[:,2]=='Iris-versicolor',2]=2\n",
    "df[df[:,2]=='Iris-virginica',2]=2\n",
    "# print(df)\n",
    "\n",
    "#计算某一属性里面的能取得最大Gain的对应的split_point，以及其gain\n",
    "def evaluate_numeric_attribute(D,attribute,sigma):\n",
    "    featureList = [instance[attribute] for instance in D]\n",
    "    uniqueVals = set(featureList)\n",
    "    uniqueVals=sorted(list(uniqueVals))\n",
    "    split_point=0.0\n",
    "    BestGain=0.0\n",
    "    for i in range(len(uniqueVals)-1):\n",
    "        v=(uniqueVals[i]+uniqueVals[i+1])/2.0\n",
    "        DY,DN=splitPointDataset(D,attribute,v)\n",
    "        if len(DY)<=sigma or len(DN)<=sigma:\n",
    "            continue\n",
    "        baseEnt = calEnt(D)\n",
    "        Gain=baseEnt-float(len(DY))/len(D)*calEnt((DY))-float(len(DN))/len(D)*calEnt(DN)\n",
    "        if Gain>BestGain:\n",
    "            BestGain=Gain\n",
    "            split_point=v\n",
    "    return split_point,BestGain\n",
    "\n",
    "\n",
    "#计算数据的类别属性里面比例最大的类别的label以及其纯度\n",
    "def majorityLabel_purity(classList):\n",
    "    if len(classList)==0:\n",
    "        return None,0.0,0\n",
    "    classCount={}\n",
    "    for n in classList:\n",
    "        if n not in classCount.keys():\n",
    "            classCount[n]=0\n",
    "        classCount[n]+=1\n",
    "    #print classCount\n",
    "    sortedCC=sorted(classCount.items(),key=lambda x:x[1],reverse=True)\n",
    "    majorityLabel=sortedCC[0][0]\n",
    "    purity=1.0*sortedCC[0][1]/len(classList)\n",
    "    num=len(classList)\n",
    "    return [majorityLabel,purity,num]\n",
    "\n",
    "#熵\n",
    "def calEnt(D):\n",
    "    num=len(D)\n",
    "    labelCount={}\n",
    "    for instance in D:\n",
    "        currentLabel=instance[-1]\n",
    "        if currentLabel not in labelCount.keys():\n",
    "            labelCount[currentLabel]=0\n",
    "        labelCount[currentLabel]+=1\n",
    "    Ent=0.0\n",
    "    for key in labelCount:\n",
    "        prob=float(labelCount[key])/num\n",
    "        Ent-=prob*math.log(prob,2)\n",
    "    return Ent\n",
    "\n",
    "#通过split_point对数据进行split处理\n",
    "def splitPointDataset(D,attribute,value):\n",
    "    DY=[]\n",
    "    DN=[]\n",
    "    for instance in D:\n",
    "        if instance[attribute]<=value:\n",
    "            DY.append(instance)\n",
    "        if instance[attribute]>value:\n",
    "            DN.append(instance)\n",
    "    return DY,DN\n",
    "\n",
    "#选择最优的属性\n",
    "def chooseBestFeature(dataSet,sigma,pai):\n",
    "    classList = [instance[-1] for instance in dataSet]\n",
    "    majorityLabel,purity,num=majorityLabel_purity(classList)\n",
    "    if len(set(classList))==1:     # 停止条件 1\n",
    "        return None,majorityLabel,purity,num               # 返回标签的多数类别作为叶子节点\n",
    "    if purity>=pai:               # 停止条件 2   如果所给的数据纯度达到标准，则停止划分\n",
    "        return None,majorityLabel,purity,num\n",
    "    featnum=len(dataSet[0])-1\n",
    "    bestFeature=-1\n",
    "    bestGain=0.0\n",
    "    bestSplit_point=0.0\n",
    "    for i in range(featnum):\n",
    "        split_point,Gain = evaluate_numeric_attribute(dataSet,i,sigma)\n",
    "        if Gain>bestGain:\n",
    "            bestGain=Gain\n",
    "            bestFeature=i\n",
    "            bestSplit_point=split_point\n",
    "    dY, dN = splitPointDataset(dataSet,bestFeature,bestSplit_point)\n",
    "    if len(dY) < sigma or len(dN) < sigma:        # 停止条件 3\n",
    "        return None,majorityLabel,purity,num\n",
    "    return bestFeature,bestSplit_point,bestGain,num\n",
    "\n",
    "#构造决策树\n",
    "def createTree(D,sigma,pai):\n",
    "    bestFeature, split_point,purity,num= chooseBestFeature(D,sigma,pai)\n",
    "    if bestFeature == None: return {'label':split_point,'purity':purity,'number':num}\n",
    "    DY, DN = splitPointDataset(D, bestFeature, split_point)\n",
    "    Decision_tree = {}\n",
    "    Decision_tree['attribute'] = bestFeature\n",
    "    Decision_tree['split_point'] = split_point\n",
    "    Decision_tree['Gain']=purity\n",
    "    Decision_tree['left'] = createTree(DY, sigma, pai)\n",
    "    Decision_tree['right'] = createTree(DN, sigma, pai)\n",
    "    return Decision_tree\n",
    "\n",
    "\n",
    "df=df.tolist()\n",
    "myTree = createTree(df,2,0.95)\n",
    "print(myTree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考\n",
    "https://github.com/nanzai9996/kernel_trick-and_denclue_and_decison_tree-/blob/master/decision_tree.py\n",
    "https://blog.csdn.net/weixin_36586536/article/details/80468426"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
